{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A simple numpy implementation of a XOR gate to understand the backpropagation\n",
    "algorithm\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "Output\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the input of the XOR gate is sorted in the matrix X and the desired output\n",
    "in matrix Y\n",
    "\"\"\"\n",
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0,1,1,0]]).T\n",
    "print(\"Input\")\n",
    "print(X)\n",
    "print(\"\\nOutput\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=X.shape[0] #initializing number of training examples\n",
    "n=X.shape[1] #initializing number of features/input\n",
    "hidden_s = 2 #initialzing hidden layer size\n",
    "l_r = 1 #initializing learning rate for backpropagation\n",
    "\n",
    "\"\"\"\n",
    "We initialize the weights that our model learns first as matrices of random\n",
    "variables which are then updated by backpropagation. \n",
    "\n",
    "generally the dimensions for each weight vector is: (size of current layer+1,size of next layer)\n",
    "\"\"\"\n",
    "theta1 = (np.random.random((n + 1, hidden_s)))\n",
    "theta2 = (np.random.random((hidden_s + 1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    the sigmoid function is used as an activation fucntion to convert linear outputs\n",
    "    to non linear outputs such that probability is outputed in between 0 and 1\n",
    "    \n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(z):\n",
    "    \"\"\"\n",
    "    the derivative of the sigmoid function is computed.\n",
    "    \"\"\"\n",
    "    s=sigmoid(z)\n",
    "    return s*(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(X,theta1,theta2):\n",
    "    \"\"\"\n",
    "    This function propagates through the network computing the output of\n",
    "    every layer of the neural network with the given inputs and weights and\n",
    "    computes the final output of the xor gate\n",
    "    \n",
    "    \"\"\"\n",
    "    #first, a column of biases is added to the input of the first layer\n",
    "    a1=np.c_[np.ones(X.shape[0]),X]#mxn+1\n",
    "    #the weights of the first layer are multiplied by the input of the first layer\n",
    "    z1=a1.dot(theta1)#mxhidden\n",
    "    #the input of the second layer is the output of the first layer, passed through the activation function and column of biases is added \n",
    "    a2=np.c_[np.ones(X.shape[0]),sigmoid(z1)]#mxhidden+1\n",
    "    #the input of the second layer is multiplied by the weights\n",
    "    z3=a2.dot(theta2)\n",
    "    #the output is passed through the activation function to obtain the final probability\n",
    "    h3=sigmoid(z3)\n",
    "    #print(h3.shape)\n",
    "    return a1,z1,a2,z3,h3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performing backpropagation\n",
    "\n",
    "For every iteration, we are calculating the error of each layer and updating the weights\n",
    "appropriately so as to minimize this error.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in range(1000):\n",
    "    a1, z1, a2, z3, hyp = forward_propagate(X, theta1, theta2)# for every iteration, forward propagation is carried out\n",
    "    del_2= Y-hyp#the error of the final layer is calculated- the difference between the predicted and actual output\n",
    "    #the error of the previous layer is found by computing the dot product of the error of the previous layer and the weights of the second layer,without the column for biases.\n",
    "    #this matrix is made to undergo element-wise multiplication with the output of the first layer(taking into account the activation function)\n",
    "    del_1=del_2.dot(theta2[1:,:].T)\n",
    "\n",
    "    #the error of the second layer is multiplied element wise by the sigmoid gradient of the output of the second layer\n",
    "    delta2=del_2\n",
    "    \n",
    "    #the error of the first layer is multiplied element wise by the sigmoid gradient of the output of the first layer\n",
    "    delta1=del_1*sigmoid_grad(z1)\n",
    "\n",
    "    #the parameters are updated using gradient descent\n",
    "    theta2+=l_r*a2.T.dot(delta2)\n",
    "    theta1+=l_r*a1.T.dot(delta1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1, z1, a2, z3, hyp = forward_propagate(X, theta1, theta2) #running forward propagation with the updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Output\n",
      "[[ 0.02060586]\n",
      " [ 0.98041979]\n",
      " [ 0.98041576]\n",
      " [ 0.01658563]]\n"
     ]
    }
   ],
   "source": [
    "#displaying the predicted output of the model\n",
    "print(\"\\nPredicted Output\")\n",
    "print(hyp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
